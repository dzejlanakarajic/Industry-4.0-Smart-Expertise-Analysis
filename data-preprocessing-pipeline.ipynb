{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfe9dfb",
   "metadata": {},
   "source": [
    "### This script contains various utility functions that import, clean and transform the data.\n",
    "Input: Raw data sources\n",
    "Output: Cleaned data sources ready for furhter analysis\n",
    "\n",
    "<b>Note:</b> Some column names and specific parts are intentionally excluded for data security reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58254d1c",
   "metadata": {},
   "source": [
    "##### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7339b9",
   "metadata": {},
   "source": [
    "##### Function for data transformation and cleaning\n",
    "\n",
    "Converting columns to correct data types and renaming columns for easier analysis. Additionally, data is cleaned by dropping inconsistent dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58545cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, emp=False, machine=False, quality=False):\n",
    "    # Convert all datetime like values to pandas datetime\n",
    "    date_columns = [###]\n",
    "    for dt in date_columns:\n",
    "        if dt in data:\n",
    "            data[dt] = pd.to_datetime(data[dt])\n",
    "    if emp:\n",
    "        data.rename(\n",
    "            columns={###}, \n",
    "                inplace=True)\n",
    "\n",
    "        data.drop(data.loc[(data.start.dt.year < 2019)].index, inplace=True)\n",
    "        data.drop(data.loc[(data.end.dt.year < 2019)].index, inplace=True)\n",
    "\n",
    "    elif machine:\n",
    "        '''Rename the columns for easier access.'''\n",
    "        data.rename(\n",
    "            columns={###},\n",
    "                inplace=True)\n",
    "\n",
    "    elif quality:\n",
    "        data.rename(\n",
    "            columns={###}, \n",
    "                inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e358956",
   "metadata": {},
   "source": [
    "##### Functions for feature extraction\n",
    "This function generates new features from date objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data, emp=False, quality=False, machine=False):\n",
    "    if machine:\n",
    "        data['start_year'] = data['start'].dt.year\n",
    "        data['end_year'] = data['end'].dt.year\n",
    "        data['start_month'] = data['start'].dt.month\n",
    "        data['end_month'] = data['end'].dt.month\n",
    "        data['start_day'] = data['start'].dt.day_name()\n",
    "        data['end_day'] = data['end'].dt.day_name()\n",
    "        data['start_hour'] = data['start'].dt.hour\n",
    "        data['end_hour'] = data['end'].dt.hour\n",
    "\n",
    "    elif emp:\n",
    "        data['start'] = data['start'].dt.round('2H')\n",
    "        data['end'] = data['end'].dt.round('2H')\n",
    "        data['start_day'] = data['start'].dt.day_name()\n",
    "        data['end_day'] = data['end'].dt.day_name()\n",
    "        data['start_hour'] = data['start'].dt.hour\n",
    "        data['end_hour'] = data['end'].dt.hour\n",
    "        data['hours_worked'] = data.end - data.start\n",
    "        data['hours_worked'] = data['hours_worked'] / np.timedelta64(1, 'h')\n",
    "        data['hours_worked'] = data['hours_worked'].astype(int)\n",
    "        data['experience'] = data.end - data.since\n",
    "        data['experience'] = data['experience'] / np.timedelta64(1, 'Y')\n",
    "        data['shift'] = data.apply(lambda row: label_shift(row), axis=1)\n",
    "\n",
    "    elif quality:\n",
    "        data['start_day'] = data['start'].dt.day_name()\n",
    "        data['start_hour'] = data['start'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a50e7c",
   "metadata": {},
   "source": [
    "This function extracts a new variable (employee shift) based on the start and end working hours from the shift model (see <b>shift-model.ipynb</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8adcd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_shift(row):\n",
    "    if (row['start_day'] == 'Monday') & (row['start_hour'] == 6) & (row['hours_worked'] == 12):\n",
    "        return 'E12'\n",
    "    if (row['start_day'] == 'Monday') & (row['start_hour'] == 18) & (row['hours_worked'] == 12):\n",
    "        return 'N12'\n",
    "    if (row['start_day'] == 'Tuesday') & (row['start_hour'] == 6) & (row['hours_worked'] == 8):\n",
    "        return 'E8'\n",
    "    if (row['start_day'] == 'Tuesday') & (row['start_hour'] == 14) & (row['hours_worked'] == 8):\n",
    "        return 'L8'\n",
    "    # other cases are excluded\n",
    "    return 'No Shift Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7c8c3",
   "metadata": {},
   "source": [
    "##### Functions to check for inconsistend dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9536a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_date_quality(df):\n",
    "    cond1 = df.start.dt.year < 2019\n",
    "\n",
    "    conditions = [cond15, cond13, cond11, cond9, cond7, cond5,\n",
    "                  cond1, cond3]\n",
    "\n",
    "    for con in conditions:\n",
    "        df.drop(df.loc[con].index, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab1e43",
   "metadata": {},
   "source": [
    "##### Main functions that trigger the pipeline and produce clean data\n",
    "Read and clean the employees dataset.\n",
    "\n",
    "<em> Employee dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1626dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_employee_data(clean=True):\n",
    "    filenames = ['Data_1_RAW.csv', 'Data_2_RAW.csv']\n",
    "    dataframes = []\n",
    "\n",
    "    for f in filenames:\n",
    "        dataframes.append(pd.read_csv(Path().joinpath('data', f), index_col=0))\n",
    "\n",
    "    if clean:\n",
    "\n",
    "        for dataframe in dataframes:\n",
    "            transform_data(dataframe, emp=True)\n",
    "            feature_extraction(dataframe, emp=True)\n",
    "\n",
    "        dataframes[0].to_csv(Path().joinpath('data', 'Data_1_CLEAN.csv'))\n",
    "        dataframes[1].to_csv(Path().joinpath('data', 'Data_2_CLEAN.csv'))\n",
    "\n",
    "        return dataframes[0], dataframes[1]\n",
    "\n",
    "    else:\n",
    "        return dataframes[0], dataframes[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726486a",
   "metadata": {},
   "source": [
    "Read and clean the industry datasets.\n",
    "\n",
    "<em>Machine error data:</em> information about machine downtimes, error codes, etc.\n",
    "\n",
    "<em>Quality control data:</em> information about quality control in the production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66380c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_machine_error_data(clean=True):\n",
    "    dataframe = pd.read_csv(Path().joinpath('data', 'Data_1_RAW.csv'))\n",
    "\n",
    "    if clean:\n",
    "        transform_data(dataframe, machine=True)\n",
    "        feature_extraction(dataframe, machine=True)\n",
    "        check_date_machine(dataframe)\n",
    "        dataframe.to_csv(Path().joinpath('data', 'Data_1_CLEAN.csv'))\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    else:\n",
    "        return dataframe\n",
    "\n",
    "def get_quality_control_data(clean=True):\n",
    "    dataframe = pd.read_csv(Path().joinpath('data', 'Data_2_RAW.csv'), index_col=0)\n",
    "\n",
    "    if clean:\n",
    "        transform_data(dataframe, quality=True)\n",
    "        feature_extraction(dataframe, quality=True)\n",
    "        check_date_quality(dataframe)\n",
    "        dataframe.to_csv(Path().joinpath('data', 'Data_2_CLEAN.csv'))\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    else:\n",
    "        return dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
