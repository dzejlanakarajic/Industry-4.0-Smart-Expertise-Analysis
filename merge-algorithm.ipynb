{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b100501",
   "metadata": {},
   "source": [
    "### Merge algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb416fd3",
   "metadata": {},
   "source": [
    "The algorithm was designed in order to combine two different data sources which are different in sizes and have no unique columns, hence why the conventional merge operations aren't suitable.\n",
    "\n",
    "<b> Use Case for the Algorithm </b>\n",
    "\n",
    "The goal is to combine production events (e.g., machine downtimes) and employees present during that event.\n",
    "\n",
    "<b>Example:</b> In the downtimes dataset, an entry is recorded about a machine with <b>ID 3</b> that stopped working at <b>2020-04-28 21:25:00</b> and was functional again at <b>2020-04-28 22:00:00</b>. In the employees dataset we can check which employees were working during that timeframe and associate them responsible for fixing the broken machine. The number of employees, their IDs and their shift are recorded for each production event as arrays during the merge process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022213de",
   "metadata": {},
   "source": [
    "##### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import numpy as np\n",
    "from scripts import utility_functions as ut #data preprocessing pipeline\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75d756",
   "metadata": {},
   "source": [
    "##### Read and clean the data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp, emp2 = ut.get_employee_data() #employee data\n",
    "exp = ut.get_machine_error_data() #downtimes\n",
    "exp2 = ut.get_quality_control_data() #quality control data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82169b61",
   "metadata": {},
   "source": [
    "##### Merge Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c115534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(x, y):\n",
    "    '''\n",
    "    :param x: Machine Dataset\n",
    "    :param y: Employee Data\n",
    "    :return: Merged Dataset\n",
    "    '''\n",
    "    df1 = x[['relevant columns from downtimes dataset']]\n",
    "    df2 = y[['relevant columns from employee dataset']]\n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    for row in df1.itertuples():\n",
    "        in_time_window = df2[\n",
    "            (row.start >= df2['start']) & (row.end <= df2['end']) # filtering all employees between events\n",
    "            ]\n",
    "\n",
    "        hashes = in_time_window['hash_code'].tolist() # an array representing a group of employees for each event\n",
    "        plan_group = in_time_window['plan_group'].tolist() # plan group of each employee\n",
    "        experiences = in_time_window['experience'].tolist() # experience in years of each employee\n",
    "        shifts = in_time_window['shift'].tolist() # shift of each employee in case an event overlaps\n",
    "        group_shift = list(dict.fromkeys(shifts)) # check wheter all employees are in the same shift\n",
    "        group_shift = \" \".join(str(x) for x in group_shift)\n",
    "        group_avg_experience = 0 # average group experience\n",
    "        if len(experiences) == 0:\n",
    "            pass # divide by zero error\n",
    "        else:\n",
    "            group_avg_experience = sum(experiences) / len(experiences)\n",
    "\n",
    "        new_rows.append( # create new dataset with selected variables\n",
    "            {\n",
    "                'start_date': row.start,\n",
    "                'end_date': row.end,\n",
    "                'hash_keys': hashes, # employee groups\n",
    "                'plan_groups': plan_group,\n",
    "                'downtime': (row.end - row.start) / np.timedelta64(1, 'h'), # recorded downtime for each event\n",
    "                'group_size': len(hashes), # group size\n",
    "                # rest are omitted\n",
    "            }\n",
    "        )\n",
    "\n",
    "    output = DataFrame(new_rows)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bab6b1a",
   "metadata": {},
   "source": [
    "##### Process parallelization using multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f5c54",
   "metadata": {},
   "source": [
    "The above algorithm's run time decreases w.r.t to the dataset size (e.g., the number of date comparisons), hence the algorithm is adapted for process parallelization using Pool object from multiprocessing module, which significantlly increased the performance of the algorithm.\n",
    "\n",
    "<b>The algorithm for quality control data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_parallelize(df: DataFrame, iterrows):\n",
    "    _, row = iterrows\n",
    "    in_time_window = df2[\n",
    "        (df2['start'] <= row['start']) & (row['start'] <= df2['end'])\n",
    "        ]\n",
    "    hashes = in_time_window['hash_code'].tolist() # employee groups\n",
    "    # rest of the generated columns are omitted\n",
    "    return {\n",
    "        'inspectionDate': row['start'],\n",
    "        'product_type': row['product_type'],\n",
    "        # rest are omitted\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e3e58",
   "metadata": {},
   "source": [
    "##### Running process parallelization and constructing pool object\n",
    "\n",
    "Pool method was chosen since it allows parallelization of a function accross different input sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4313266",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing.set_start_method(\"fork\")\n",
    "st = time.time()\n",
    "func = partial(to_parallelize, df2)\n",
    "rows = df1.iterrows()\n",
    "\n",
    "pool = Pool(processes=4)\n",
    "new_rows2 = pool.map(func, rows)\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(time.time() - st)  # 59 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38647c1",
   "metadata": {},
   "source": [
    "The average runtime wasn't tested but the algorithm without parallelization ran around 1hr and during this test it completed in 59 seconds with process based parallelism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
